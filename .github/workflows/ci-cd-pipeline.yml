name: Lugx Gaming CI/CD Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run integration tests every 4 hours for reliability monitoring
    - cron: '0 */4 * * *'

env:
  DOCKER_REGISTRY: docker.io
  DOCKER_USERNAME: spmani99
  AWS_REGION: us-east-1
  EKS_CLUSTER_NAME: lugx-gaming-cluster
  
jobs:
  # Job 1: Detect Changed Services
  detect-changes:
    name: Detect Changed Services
    runs-on: ubuntu-latest
    outputs:
      frontend-changed: ${{ steps.changes.outputs.frontend }}
      game-service-changed: ${{ steps.changes.outputs.game-service }}
      order-service-changed: ${{ steps.changes.outputs.order-service }}
      analytics-service-changed: ${{ steps.changes.outputs.analytics-service }}
      any-changes: ${{ steps.changes.outputs.any }}
      
    steps:
    - name: Checkout Code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Get full history for change detection
        
    - name: Detect Service Changes
      id: changes
      run: |
        echo "Detecting which services have changed..."
        
        # Get list of changed files
        CHANGED_FILES=$(git diff --name-only ${{ github.event.before }} ${{ github.event.after }} 2>/dev/null || git diff --name-only HEAD~1 HEAD)
        
        echo "Changed files:"
        echo "$CHANGED_FILES"
        
        # Check which services have changes
        FRONTEND_CHANGED=false
        GAME_SERVICE_CHANGED=false
        ORDER_SERVICE_CHANGED=false
        ANALYTICS_SERVICE_CHANGED=false
        
        if echo "$CHANGED_FILES" | grep -q "^frontend/"; then
          FRONTEND_CHANGED=true
          echo "Frontend has changes"
        fi
        
        if echo "$CHANGED_FILES" | grep -q "^game-service/"; then
          GAME_SERVICE_CHANGED=true
          echo "Game Service has changes"
        fi
        
        if echo "$CHANGED_FILES" | grep -q "^order-service/"; then
          ORDER_SERVICE_CHANGED=true
          echo "Order Service has changes"
        fi
        
        if echo "$CHANGED_FILES" | grep -q "^analytics-service/"; then
          ANALYTICS_SERVICE_CHANGED=true
          echo "Analytics Service has changes"
        fi
        
        # For scheduled runs or if no specific changes detected, build all
        if [ "${{ github.event_name }}" = "schedule" ] || [ -z "$CHANGED_FILES" ]; then
          FRONTEND_CHANGED=true
          GAME_SERVICE_CHANGED=true
          ORDER_SERVICE_CHANGED=true
          ANALYTICS_SERVICE_CHANGED=true
          echo "Scheduled run or no changes detected - building all services"
        fi
        
        # Set outputs
        echo "frontend=$FRONTEND_CHANGED" >> $GITHUB_OUTPUT
        echo "game-service=$GAME_SERVICE_CHANGED" >> $GITHUB_OUTPUT
        echo "order-service=$ORDER_SERVICE_CHANGED" >> $GITHUB_OUTPUT
        echo "analytics-service=$ANALYTICS_SERVICE_CHANGED" >> $GITHUB_OUTPUT
        echo "any=true" >> $GITHUB_OUTPUT

  # Job 2: Build and Test (Selective)
  build-and-test:
    name: Build & Test Services
    runs-on: ubuntu-latest
    needs: detect-changes
    if: needs.detect-changes.outputs.any-changes == 'true'
    
    strategy:
      matrix:
        service: [frontend, game-service, order-service, analytics-service]
        include:
          - service: frontend
            should-build: ${{ needs.detect-changes.outputs.frontend-changed }}
          - service: game-service
            should-build: ${{ needs.detect-changes.outputs.game-service-changed }}
          - service: order-service
            should-build: ${{ needs.detect-changes.outputs.order-service-changed }}
          - service: analytics-service
            should-build: ${{ needs.detect-changes.outputs.analytics-service-changed }}
        
    outputs:
      frontend-image: ${{ steps.meta.outputs.tags }}
      game-service-image: ${{ steps.meta.outputs.tags }}
      order-service-image: ${{ steps.meta.outputs.tags }}
      analytics-service-image: ${{ steps.meta.outputs.tags }}
      
    steps:
    - name: Checkout Code
      uses: actions/checkout@v4
      
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
      
    - name: Login to Docker Hub
      uses: docker/login-action@v3
      with:
        username: ${{ env.DOCKER_USERNAME }}
        password: ${{ secrets.DOCKER_PASSWORD }}
        
    - name: Extract Metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.DOCKER_USERNAME }}/${{ matrix.service }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix={{branch}}-
          type=raw,value=latest,enable={{is_default_branch}}
          
    - name: Build and Push Docker Image
      if: matrix.should-build == 'true'
      uses: docker/build-push-action@v5
      with:
        context: ./${{ matrix.service }}
        platforms: linux/amd64
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max
        
    - name: Skip Build (No Changes)
      if: matrix.should-build != 'true'
      run: |
        echo "Skipping build for ${{ matrix.service }} - no changes detected"
        echo "Using existing image: ${{ env.DOCKER_USERNAME }}/${{ matrix.service }}:latest"
        
    - name: Run Unit Tests
      if: matrix.should-build == 'true'
      run: |
        echo "Running unit tests for ${{ matrix.service }}"
        # Run tests on source code for all services
        cd ${{ matrix.service }}
        npm install
        npm test
        
    - name: Skip Tests (No Changes)
      if: matrix.should-build != 'true'
      run: |
        echo "Skipping tests for ${{ matrix.service }} - no changes detected"

  # Job 3: Security Scanning (Selective)
  security-scan:
    name: Security Scanning
    runs-on: ubuntu-latest
    needs: [detect-changes, build-and-test]
    if: needs.detect-changes.outputs.any-changes == 'true'
    
    strategy:
      matrix:
        service: [frontend, game-service, order-service, analytics-service]
        include:
          - service: frontend
            should-scan: ${{ needs.detect-changes.outputs.frontend-changed }}
          - service: game-service
            should-scan: ${{ needs.detect-changes.outputs.game-service-changed }}
          - service: order-service
            should-scan: ${{ needs.detect-changes.outputs.order-service-changed }}
          - service: analytics-service
            should-scan: ${{ needs.detect-changes.outputs.analytics-service-changed }}
        
    steps:
    - name: Checkout Code
      uses: actions/checkout@v4
      
    - name: Run Trivy Vulnerability Scanner
      if: matrix.should-scan == 'true'
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: ${{ env.DOCKER_USERNAME }}/${{ matrix.service }}:latest
        format: 'sarif'
        output: 'trivy-results.sarif'
        
    - name: Upload Trivy Scan Results
      if: matrix.should-scan == 'true'
      uses: github/codeql-action/upload-sarif@v3
      continue-on-error: true
      with:
        sarif_file: 'trivy-results.sarif'
        
    - name: Skip Security Scan (No Changes)
      if: matrix.should-scan != 'true'
      run: |
        echo "Skipping security scan for ${{ matrix.service }} - no changes detected"

  # Job 4: Deploy to Production (Rolling Deployment for 100% Uptime)
  deploy-production:
    name: Deploy to Production (Rolling)
    runs-on: ubuntu-latest
    needs: [build-and-test, security-scan]
    if: github.ref == 'refs/heads/main'
    environment: production
    
    steps:
    - name: Checkout Code
      uses: actions/checkout@v4
      
    - name: Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
        
    - name: Setup kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'latest'
        
    - name: Connect to EKS Cluster
      run: |
        aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}
        
    - name: Rolling Deployment to Production (100% Uptime)
      run: |
        echo "Starting rolling deployment to production for 100% uptime..."
        
        # Apply all Kubernetes manifests to default namespace
        echo "Applying Kubernetes manifests with rolling update strategy..."
        kubectl apply -f frontend/frontend-deployment.yaml
        kubectl apply -f frontend/frontend-secret.yaml
        kubectl apply -f game-service/game-service-deployment.yaml
        kubectl apply -f game-service/game-db-secret.yaml
        kubectl apply -f order-service/order-service-deployment.yaml
        kubectl apply -f order-service/order-db-secret.yaml
        kubectl apply -f analytics-service/analytics-service-deployment.yaml
        kubectl apply -f analytics-service/analytics-secret.yaml
        
        # Apply main ingress
        echo "Applying main ingress..."
        kubectl apply -f ingress.yaml
        
        # Update image tags in deployment files
        services=("frontend" "game-service" "order-service" "analytics-service")
        
        for service in "${services[@]}"; do
          echo "Deploying $service to production..."
          
          # Update deployment with new image
          kubectl set image deployment/${service}-deployment ${service}=${{ env.DOCKER_USERNAME }}/${service}:${{ github.sha }}
          
          # Force rollout without waiting for graceful termination
          kubectl rollout status deployment/${service}-deployment -n lugx-staging --timeout=60s || true
          kubectl delete pods -l app=${service} -n lugx-staging --force --grace-period=0 || true
          
          # Check deployment health
          kubectl get pods -l app=${service}
        done
        
        echo "Production deployment completed!"
        echo "Production URL: http://lugx-games.local"
        
    - name: Run Integration Tests - Production
      run: |
        aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}
        
    - name: 🔵 Blue-Green Deployment Setup
      run: |
        echo "🔵 Setting up Blue-Green deployment..."
        
        # Determine current active environment
        if kubectl get namespace lugx-blue > /dev/null 2>&1; then
          CURRENT_ENV="blue"
          NEW_ENV="green" 
        else
          CURRENT_ENV="green"
          NEW_ENV="blue"
        fi
        
        echo "📊 Current active: lugx-$CURRENT_ENV"
        echo "🆕 Deploying to: lugx-$NEW_ENV"
        echo "CURRENT_ENV=$CURRENT_ENV" >> $GITHUB_ENV
        echo "NEW_ENV=$NEW_ENV" >> $GITHUB_ENV
        
    - name: 🏗️ Deploy to New Environment
      run: |
        echo "🚀 Deploying to lugx-${{ env.NEW_ENV }}..."
        
        # Create new environment namespace
        kubectl create namespace lugx-${{ env.NEW_ENV }} --dry-run=client -o yaml | kubectl apply -f -
        
        # Apply secrets to new environment
        kubectl apply -f frontend/frontend-secret.yaml -n lugx-${{ env.NEW_ENV }}
        kubectl apply -f game-service/game-db-secret.yaml -n lugx-${{ env.NEW_ENV }}
        kubectl apply -f order-service/order-db-secret.yaml -n lugx-${{ env.NEW_ENV }}
        kubectl apply -f analytics-service/analytics-secret.yaml -n lugx-${{ env.NEW_ENV }}
        
        # Deploy services with updated images
        services=("frontend" "game-service" "order-service" "analytics-service")
        
        for service in "${services[@]}"; do
          echo "📦 Deploying $service to lugx-${{ env.NEW_ENV }}..."
          
          # Create temporary deployment file with updated image and namespace
          sed "s|image: spmani99/${service}:latest|image: spmani99/${service}:${{ github.sha }}|g" ${service}/${service}-deployment.yaml > temp-${service}-deployment.yaml
          sed -i "s|namespace: default|namespace: lugx-${{ env.NEW_ENV }}|g" temp-${service}-deployment.yaml
          
          kubectl apply -f temp-${service}-deployment.yaml
          
          # Wait for deployment to be ready
          kubectl rollout status deployment/${service}-deployment -n lugx-${{ env.NEW_ENV }} --timeout=60s || true
          kubectl delete pods -l app=${service} -n lugx-${{ env.NEW_ENV }} --force --grace-period=0 || true
          
          rm temp-${service}-deployment.yaml
        done
        
    - name: 🧪 Run Integration Tests - Production
      run: |
        echo "🧪 Running integration tests on new production environment..."
        chmod +x .github/scripts/integration-tests.sh
        .github/scripts/integration-tests.sh production

  # Job 5: Periodic Integration Tests (Reliability Monitoring)
  periodic-integration-tests:
    name: Periodic Integration Tests
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'
    
    steps:
    - name: Checkout Code
      uses: actions/checkout@v4
      
    - name: Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
        
    - name: Setup kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'latest'
        
    - name: Connect to EKS Cluster
      run: |
        aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}
        
    - name: Running Periodic Integration Tests
      run: |
        echo "Running periodic integration tests for reliability monitoring..."
        chmod +x .github/scripts/integration-tests.sh
        .github/scripts/periodic-tests.sh
        
    - name: Generate Test Report
      run: |
        echo "Generating reliability test report..."
        chmod +x .github/scripts/generate-report.sh
        .github/scripts/generate-report.sh
        
    - name: Upload Test Results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: periodic-test-results
        path: test-results/
        
    - name: Log Test Failure
      if: failure()
      run: |
        echo "Periodic integration tests failed!"
        echo "Check the workflow logs and test results for details."
        echo "Failed at: $(date)"

  # Job 7: Rollback on Failure
  rollback:
    name: Rollback on Failure
    runs-on: ubuntu-latest
    needs: [deploy-production]
    if: failure() && github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout Code
      uses: actions/checkout@v4
      
    - name: Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
        
    - name: Setup kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'latest'
        
    - name: Connect to EKS Cluster
      run: |
        aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}
        
    - name: Execute Rollback
      run: |
        echo "Executing rollback to previous stable version..."
        chmod +x .github/scripts/rollback.sh
        .github/scripts/rollback.sh
        
    - name: Log Rollback Completion
      run: |
        echo "Production deployment failed. Rollback executed to previous stable version."
        echo "Check the deployment logs for failure details."
        echo "Rollback completed at: $(date)"